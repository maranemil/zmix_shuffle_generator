{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e61d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch   -q\n",
    "!pip install torchaudio -q\n",
    "!pip install boto3 -q\n",
    "!pip install matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import io\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from IPython.display import Audio\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb325c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/audio/blob/main/examples/tutorials/audio_io_tutorial.py\n",
    "# https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#loading-audio-data-into-tensor\n",
    "# https://developers.deepgram.com/blog/2022/06/pytorch-intro-with-torchaudio/\n",
    "# https://pytorch.org/audio/0.11.0/tutorials/audio_io_tutorial.html\n",
    "# https://pytorch.org/audio/main/tutorials/audio_io_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/audio_resampling_tutorial.html\n",
    "# https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#applying-effects-and-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45f143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ebdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "  if src:\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", src)\n",
    "    print(\"-\" * 10)\n",
    "  if sample_rate:\n",
    "    print(\"Sample Rate:\", sample_rate)\n",
    "  print(\"Shape:\", tuple(waveform.shape))\n",
    "  print(\"Dtype:\", waveform.dtype)\n",
    "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "  print()\n",
    "  print(waveform)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "    axes[c].grid(True)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "    if ylim:\n",
    "      axes[c].set_ylim(ylim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "    \n",
    "def play_audio(waveform, sample_rate):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  if num_channels == 1:\n",
    "    display(Audio(waveform[0], rate=sample_rate))\n",
    "  elif num_channels == 2:\n",
    "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "  else:\n",
    "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77785f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# SAMPLE_WAV = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\n",
    "# metadata = torchaudio.info(SAMPLE_WAV)\n",
    "# print(metadata)\n",
    "# waveform, sample_rate = torchaudio.load(SAMPLE_WAV)\n",
    "\n",
    "\n",
    "# https://www.projectpro.io/recipes/load-audio-file-pytorch\n",
    "# https://www.geeksforgeeks.org/loading-data-in-pytorch/\n",
    "audio_file = \"out2.wav\"\n",
    "waveform, sample_rate  = torchaudio.load(audio_file)\n",
    "#print_stats(waveform, sample_rate=sample_rate)\n",
    "plot_waveform(waveform, sample_rate)\n",
    "#plot_specgram(waveform, sample_rate)\n",
    "play_audio(waveform, sample_rate)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa -q\n",
    "!pip install pydub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c92686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "DEFAULT_OFFSET = 201\n",
    "\n",
    "\n",
    "def _get_log_freq(sample_rate, max_sweep_rate, offset):\n",
    "    \"\"\"Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]\n",
    "\n",
    "    offset is used to avoid negative infinity `log(offset + x)`.\n",
    "\n",
    "    \"\"\"\n",
    "    start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)\n",
    "    return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset\n",
    "\n",
    "\n",
    "def _get_inverse_log_freq(freq, sample_rate, offset):\n",
    "    \"\"\"Find the time where the given frequency is given by _get_log_freq\"\"\"\n",
    "    half = sample_rate // 2\n",
    "    return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))\n",
    "\n",
    "\n",
    "def _get_freq_ticks(sample_rate, offset, f_max):\n",
    "    # Given the original sample rate used for generating the sweep,\n",
    "    # find the x-axis value where the log-scale major frequency values fall in\n",
    "    time, freq = [], []\n",
    "    for exp in range(2, 5):\n",
    "        for v in range(1, 10):\n",
    "            f = v * 10**exp\n",
    "            if f < sample_rate // 2:\n",
    "                t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate\n",
    "                time.append(t)\n",
    "                freq.append(f)\n",
    "    t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate\n",
    "    time.append(t_max)\n",
    "    freq.append(f_max)\n",
    "    return time, freq\n",
    "\n",
    "\n",
    "def get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):\n",
    "    max_sweep_rate = sample_rate\n",
    "    freq = _get_log_freq(sample_rate, max_sweep_rate, offset)\n",
    "    delta = 2 * math.pi * freq / sample_rate\n",
    "    cummulative = torch.cumsum(delta, dim=0)\n",
    "    signal = torch.sin(cummulative).unsqueeze(dim=0)\n",
    "    return signal\n",
    "\n",
    "\n",
    "def plot_sweep(\n",
    "    waveform,\n",
    "    sample_rate,\n",
    "    title,\n",
    "    max_sweep_rate=48000,\n",
    "    offset=DEFAULT_OFFSET,\n",
    "):\n",
    "    x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]\n",
    "    y_ticks = [1000, 5000, 10000, 20000, sample_rate // 2]\n",
    "\n",
    "    time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)\n",
    "    freq_x = [f if f in x_ticks and f <= max_sweep_rate // 2 else None for f in freq]\n",
    "    freq_y = [f for f in freq if f in y_ticks and 1000 <= f <= sample_rate // 2]\n",
    "\n",
    "    figure, axis = plt.subplots(1, 1)\n",
    "    _, _, _, cax = axis.specgram(waveform[0].numpy(), Fs=sample_rate)\n",
    "    plt.xticks(time, freq_x)\n",
    "    plt.yticks(freq_y, freq_y)\n",
    "    axis.set_xlabel(\"Original Signal Frequency (Hz, log scale)\")\n",
    "    axis.set_ylabel(\"Waveform Frequency (Hz)\")\n",
    "    axis.xaxis.grid(True, alpha=0.67)\n",
    "    axis.yaxis.grid(True, alpha=0.67)\n",
    "    figure.suptitle(f\"{title} (sample rate: {sample_rate} Hz)\")\n",
    "    plt.colorbar(cax)\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1428346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://numpy.org/doc/stable/reference/generated/numpy.repeat.html\n",
    "sample_rate = 48000\n",
    "audio_file = \"out.wav\"\n",
    "waveform, sample_rate  = torchaudio.load(audio_file)\n",
    "#waveform = get_sine_sweep(sample_rate)\n",
    "#waveform2 = waveform.repeat(3,5)\n",
    "plot_sweep(waveform, sample_rate, title=\"Original Waveform\")\n",
    "Audio(waveform.numpy()[0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define effects\n",
    "# https://pytorch.org/audio/stable/sox_effects.html\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.split.html\n",
    "# https://realpython.com/playing-and-recording-sound-python/\n",
    "# https://librosa.org/doc/latest/feature.html\n",
    "# https://github.com/pytorch/audio/issues/260\n",
    "# https://pynative.com/python-range-for-float-numbers/\n",
    "# https://stackoverflow.com/questions/6088077/how-to-get-a-random-number-between-a-float-range\n",
    "# https://linux.die.net/man/1/soxexam\n",
    "# https://pyra-handheld.com/boards/threads/help-with-using-sox-as-a-voice-changer.98878/\n",
    "# https://pip.pypa.io/en/stable/cli/pip/\n",
    "# https://pytorch.org/audio/stable/tutorials/audio_io_tutorial.html\n",
    "# https://github.com/pytorch/audio/issues/260\n",
    "# http://sox.sourceforge.net/sox.html\n",
    "# https://stackoverflow.com/questions/2890703/how-to-join-two-wav-files-using-python\n",
    "# https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html\n",
    "# https://stackoverflow.com/questions/43679631/python-how-to-change-audio-volume\n",
    "# https://github.com/jiaaro/pydub/blob/master/API.markdown\n",
    "# https://audiosegment.readthedocs.io/en/latest/audiosegment.html\n",
    "# http://man.hubwiz.com/docset/torchaudio.docset/Contents/Resources/Documents/transforms.html\n",
    "\n",
    "#del waveform3 \n",
    "\n",
    "effects = [\n",
    "  ['rate', '48000'],  # resample to 32000 Hz \n",
    "  ['remix','1,2i 1,2i'], # oops effect. 1,2i 1,2i\n",
    "  #['flanger','9.99'],\n",
    "  #['phaser','0.099'],\n",
    "  #[\"reverb\", \"1\"],  # Reverbration 0- 100 \n",
    "  #['speed','0.970'],\n",
    "  #['highpass',\"-1\",'60'],\n",
    "  #[\"speed\", str(random.uniform(0.8, 1.2))],  # reduce the speed\n",
    "  ['overdrive','3.9'],\n",
    "  ['vol','0.2'],\n",
    "  ['treble', '+4'],  \n",
    "  ['bass', '+4'], \n",
    "  #['stretch','0.79'],\n",
    "  #['tempo','1.07'], \n",
    "  #[\"lowpass\", \"-1\", \"450\"],  \n",
    "  #['remix', '-'],  # merge all the channels\n",
    "  ['gain', '-2.4'],\n",
    "  #['gain', '-n'],  # normalises to 0dB   \n",
    "  #['pad', '0', '1.5'],  # add 1.5 seconds silence at the end\n",
    "  #['trim', '0', '1'],  # get the first 2 seconds \n",
    "]\n",
    "waveform3, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n",
    "    waveform, sample_rate, effects)\n",
    "#plot_waveform(waveform3, sample_rate, title=\"Original\", xlim=(-.1, 3.2))\n",
    "#print_stats(waveform3, sample_rate=sample_rate, src=\"Original\")\n",
    "#plot_sweep(waveform3[:2], sample_rate, title=\"Original Waveform\")\n",
    "#Audio(waveform3.numpy()[0], rate=sample_rate)\n",
    "play_audio(waveform3, sample_rate)\n",
    "torchaudio.save('waveform3.wav', waveform3, sample_rate)\n",
    "# waveform3, sample_rate = torchaudio.load('waveform3.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audio(waveform4.numpy()[0], rate=sample_rate)\n",
    "effects2 = [\n",
    "  ['rate', '48000'],  # resample to 32000 Hz \n",
    "  ['remix','1,2i'], # oops effect. 1,2i 1,2i\n",
    "  #['flanger','9.99'],\n",
    "  #['phaser','0.099'],\n",
    "  #['speed','0.970'],\n",
    "  ['overdrive','7.9'],\n",
    "  #['vol','0.2'],\n",
    "  ['gain', '-2.4'],\n",
    "]\n",
    "waveform4, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n",
    "    waveform, sample_rate, effects2)\n",
    "play_audio(waveform4, sample_rate)\n",
    "torchaudio.save('waveform4.wav', waveform4, sample_rate)\n",
    "# waveform4, sample_rate = torchaudio.load('waveform4.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac414d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "sound1 = AudioSegment.from_file(\"waveform3.wav\")\n",
    "sound2 = AudioSegment.from_file(\"waveform4.wav\")\n",
    "sound3 = sound1.set_channels(1)\n",
    "sound1 = sound1.apply_gain(-6.0).apply_gain_stereo(-1, +3).pan(-0.1)\n",
    "sound2 = sound2.apply_gain(-6.0).apply_gain_stereo(+3, -1).pan(-0.8)\n",
    "sound3 = sound3.apply_gain(-12.0)\n",
    "#play(sound1)\n",
    "from IPython.display import Audio, display\n",
    "combined = sound2.overlay(sound1)\n",
    "combined = combined.overlay(sound3)\n",
    "combined.apply_gain(+16.0).export(\"combined.wav\", format='wav')\n",
    "Audio(\"combined.wav\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "sound1 = AudioSegment.from_file(\"waveform3.wav\")\n",
    "sound2 = AudioSegment.from_file(\"waveform4.wav\")\n",
    "sound3 = sound1.set_channels(1)\n",
    "sound1 = sound1.apply_gain(-8.0).apply_gain_stereo(-1, +3).pan(-0.1)\n",
    "sound2 = sound2.apply_gain(-11.0).apply_gain_stereo(+3, -1).pan(-0.8)\n",
    "sound3 = sound3.apply_gain(-2.0)\n",
    "#play(sound1)\n",
    "from IPython.display import Audio, display\n",
    "combined = sound2.overlay(sound1)\n",
    "combined = combined.overlay(sound3)\n",
    "combined.apply_gain(+16.0).export(\"combined2.wav\", format='wav')\n",
    "Audio(\"combined2.wav\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://publish.illinois.edu/augmentedlistening/tutorials/music-processing/tutorial-1-introduction-to-audio-processing-in-python/\n",
    "https://colab.research.google.com/github/pytorch/audio/blob/gh-pages/main/_downloads/08314ca72c2aad9b7951279f0a24a983/audio_data_augmentation_tutorial.ipynb\n",
    "https://www.tensorflow.org/tutorials/audio/simple_audio\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ce691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bbc4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9219f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661dd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
